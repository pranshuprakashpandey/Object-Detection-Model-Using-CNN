{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Lambda, Conv2D\n",
    "from keras.models import load_model, Model\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from YAD2K.yad2k.models.keras_yolo import preprocess_true_boxes, yolo_body, yolo_head, yolo_loss, yolo_boxes_to_corners\n",
    "from YAD2K.yad2k.utils.draw_boxes import draw_boxes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "%matplotlib inline\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the labels\n",
    "#PATH = 'C:/FluxAuto/berk_data/images/100k/train/'\n",
    "labels = pd.read_csv('labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default anchor boxes\n",
    "YOLO_ANCHORS = np.array(\n",
    "    ((0.57273, 0.677385), (1.87446, 2.06253), (3.33843, 5.47434),\n",
    "     (7.88282, 3.52778), (9.77052, 9.16828)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(classpath):\n",
    "    \"\"\"\n",
    "    Loads the classes stored in the classes.txt file.\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    List containing the classes\n",
    "    \"\"\"\n",
    "    with open(classpath) as f:\n",
    "        class_names = f.readlines()\n",
    "    class_names = class_names[0].split(' ')\n",
    "    return class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bike', 'bus', 'car', 'motor', 'person', 'rider', 'train', 'truck']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_classes('data/classes.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anchors(anchor_path):\n",
    "    \"\"\"\n",
    "    Loads the anchors from a file\n",
    "    \"\"\"\n",
    "    if os.path.isfile(anchor_path):\n",
    "        with open(anchor_path) as f:\n",
    "            anchors = f.readline()\n",
    "            anchors = [float(x) for x in anchors.split(',')]\n",
    "            return np.array(anchors).reshape(-1, 2)\n",
    "    else:\n",
    "        Warning('Could not open anchors file, using default')\n",
    "        return YOLO_ANCHORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxes(box_path):\n",
    "    \"\"\"\n",
    "    Loads the saved box coordinates\n",
    "    \"\"\"\n",
    "    load_boxes = np.load(box_path)\n",
    "    for i in load_boxes.iteritems():\n",
    "        boxes = i\n",
    "    boxes = list(boxes[1])\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold=0.6):\n",
    "    \"\"\"\n",
    "    Filters YOLO boxes by thresholding on object and class confidence.\n",
    "    Arguements: \n",
    "    box_confidence: Probability of the box containing the object.\n",
    "    boxes: The box parameters : (x, y, h, w) \n",
    "           x, y -> Center of the box \n",
    "           h, w -> Height and width of the box w.r.t the image size.\n",
    "    box_class_probs: Probability of all the classes for each box.\n",
    "    threshold: Threshold value for box confidence. \n",
    "    \n",
    "    Returns: \n",
    "    scores: containing the class probability score for the selected boxes.\n",
    "    boxes: contains box coordinates for the selected boxes.\n",
    "    classes: contains the index of the class detected by the selected boxes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the box scores: \n",
    "    box_scores = box_confidence * box_class_probs\n",
    "    \n",
    "    # Find the box classes index with the maximum box score\n",
    "    box_classes = K.argmax(box_scores)\n",
    "    # Find the box classes with maximum box score\n",
    "    box_class_scores = K.max(box_scores, axis=-1)\n",
    "    \n",
    "    # Creating a mask for selecting the boxes that have box score greater than threshold.\n",
    "    thresh_mask = box_class_scores >= threshold\n",
    "    # Selecting the scores, boxes and classes with box score greater than \n",
    "    # threshold by filtering the box score with the help of thresh_mask.\n",
    "    scores = tf.boolean_mask(tensor=box_class_scores, mask=thresh_mask)\n",
    "    classes = tf.boolean_mask(tensor=box_classes, mask=thresh_mask)\n",
    "    boxes = tf.boolean_mask(tensor=boxes, mask=thresh_mask)\n",
    "    \n",
    "    return scores, classes, boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(images, boxes=None):\n",
    "    \"\"\"\n",
    "    Process the data\n",
    "    \"\"\"\n",
    "    images = [PIL.Image.fromarray(i) for i in images]\n",
    "    orig_size = np.array([images[0].width, images[0].height])\n",
    "    orig_size = np.expand_dims(orig_size, axis=0)\n",
    "    \n",
    "    #Image preprocessing \n",
    "    processed_images = [i.resize((416, 416), PIL.Image.BICUBIC) for i in images]\n",
    "    processed_images = [np.array(image, dtype=np.float) for image in processed_images]\n",
    "    processed_images = [image/255. for image in processed_images]\n",
    "    \n",
    "    if boxes is not None:\n",
    "        # Box preprocessing\n",
    "        # Original boxes stored as as 1D list of class, x_min, y_min, x_max, y_max\n",
    "        boxes = [box.reshape((-1, 5)) for box in boxes]\n",
    "        # Get extents as y_min, x_min, y_max, x_max, class fpr comparision with \n",
    "        # model output\n",
    "        box_extents = [box[:, [2,1,4,3,0]] for box in boxes]\n",
    "        \n",
    "        # Get box parametes as x_center, y_center, box_width, box_height, class.\n",
    "        boxes_xy = [0.5* (box[:, 3:5] + box[:, 1:3]) for box in boxes]\n",
    "        boxes_wh = [box[:, 3:5] - box[:, 1:3] for box in boxes]\n",
    "        boxes_xy = [box_xy / orig_size for box_xy in boxes_xy]\n",
    "        boxes_wh = [box_wh / orig_size for box_wh in boxes_wh]\n",
    "        boxes = [np.concatenate((boxes_xy[i], boxes_wh[i], box[:, 0:1]), axis=-1) for i, box in enumerate(boxes)]\n",
    "        \n",
    "        # find the max number of boxes \n",
    "        max_boxes = 0\n",
    "        for boxz in boxes:\n",
    "            if boxz.shape[0] > max_boxes:\n",
    "                max_boxes = boxz.shape[0]\n",
    "        # add zero pad for training \n",
    "        for i, boxz in enumerate(boxes):\n",
    "            if boxz.shape[0] <  max_boxes:\n",
    "                zero_padding = np.zeros((max_boxes - boxz.shape[0], 5), dtype=np.float32)\n",
    "                boxes[i] = np.vstack((boxz, zero_padding))\n",
    "        \n",
    "        return np.array(processed_images), np.array(boxes)\n",
    "    else:\n",
    "        return np.array(processed_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detector_mask(boxes, anchors):\n",
    "    \"\"\"\n",
    "    Precompute detectors_mask and matching_true_boxes for training. \n",
    "    Detectors mask is 1 for each spatial position in the final conv layer and \n",
    "    anchor that should be active for the given boxes and 0 otherwise. \n",
    "    Matching true boxes gives the regression targets for the ground truth box that \n",
    "    caused a detector to be active or 0 otherwise.\n",
    "    \"\"\"\n",
    "    detectors_mask = [0 for i in range(len(boxes))]\n",
    "    matching_true_boxes = [0 for i in range(len(boxes))]\n",
    "    for i, box in enumerate(boxes):\n",
    "        detectors_mask[i], matching_true_boxes[i] = preprocess_true_boxes(box, anchors, [416, 416])\n",
    "    return np.array(detectors_mask), np.array(matching_true_boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(anchors, class_names, load_pretrained=True, freeze_body = True):\n",
    "    \"\"\"\n",
    "    \n",
    "    load_pretrained: whether or not to load the pretrained model or initialize all weights\n",
    "\n",
    "    freeze_body: whether or not to freeze all weights except for the last layer's\n",
    "    \n",
    "    Returns:\n",
    "    model_body : YOLOv2 with new output layer\n",
    "    model : YOLOv2 with custom loss Lambda layer  \n",
    "    \n",
    "    \"\"\"\n",
    "    detector_mask_shape = (13, 13, 5, 1)\n",
    "    matching_boxes_shape = (13, 13, 5, 5)\n",
    "    \n",
    "    # Create model input layers \n",
    "    image_input = Input(shape=(416,416,3))\n",
    "    boxes_input = Input(shape=(None, 5))\n",
    "    detector_mask_input = Input(shape=detector_mask_shape)\n",
    "    matching_boxes_input = Input(shape=matching_boxes_shape)\n",
    "    \n",
    "    # Create model body\n",
    "    yolo_model = yolo_body(image_input, len(anchors), len(class_names))\n",
    "    topless_yolo = Model(yolo_model.input, yolo_model.layers[-2].output)\n",
    "    \n",
    "    if load_pretrained == True:\n",
    "        # Save topless yolo\n",
    "        topless_yolo_path = os.path.join('model_data', 'yolo_topless.h5')\n",
    "        if not os.path.exists(topless_yolo_path):\n",
    "            print('Creating Topless weights file')\n",
    "            yolo_path = os.path.join('model_data', 'yolo.h5')\n",
    "            model_body = load_model(yolo_path)\n",
    "            model_body = Model(model_body.inputs, model_body.layers[-2].output)\n",
    "            model_body.save_weights(topless_yolo_path)\n",
    "        topless_yolo.load_weights(topless_yolo_path)\n",
    "        \n",
    "    if freeze_body:\n",
    "        for layer in topless_yolo.layers:\n",
    "            layer.trainable = False\n",
    "    \n",
    "    final_layer = Conv2D(len(anchors)*(5 + len(class_names)), (1, 1), activation='linear')(topless_yolo.output)\n",
    "    model_body = Model(image_input, final_layer)\n",
    "    \n",
    "    # Place model loss on CPU to reduce GPU memory usage.    \n",
    "    with tf.device('/cpu:0'):\n",
    "        model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss', arguments={\n",
    "            'anchors': anchors, \n",
    "            'num_classes': len(class_names)})([model_body.output, boxes_input, detector_mask_input, matching_boxes_input])\n",
    "    \n",
    "    model = Model([model_body.input, boxes_input, detector_mask_input, matching_boxes_input], model_loss)\n",
    "    return model_body, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, class_name, anchors, image_data, boxes, detectors_mask, matching_true_boxes, validation_split=0.1, epochs = 5):\n",
    "    \"\"\"\n",
    "    Trains the model and saves the weights with the lowest loss value.\n",
    "    \"\"\"\n",
    "    model.compile(optimizer='adam', loss={'yolo_loss': lambda y_true, y_pred: y_pred})\n",
    "    \n",
    "    logging = TensorBoard()\n",
    "    checkpoint = ModelCheckpoint('model_data/model.best.h5', monitor='val_loss', save_weights_only=True, save_best_only=True)\n",
    "    earlystopping = EarlyStopping(monitor='val_loss', patience=15, verbose=1)\n",
    "    \n",
    "    model.fit([image_data, boxes, detectors_mask, matching_true_boxes], np.zeros(len(image_data)), validation_split=validation_split, batch_size=16, epochs=epochs, callbacks=[logging, checkpoint, earlystopping])\n",
    "    model.save_weights('model_data/model.best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold=0.6):\n",
    "    \"\"\"\n",
    "    Filters YOLO boxes by thresholding on object and class confidence.\n",
    "    Arguements: \n",
    "    box_confidence: Probability of the box containing the object.\n",
    "    boxes: The box parameters : (x, y, h, w) \n",
    "           x, y -> Center of the box \n",
    "           h, w -> Height and width of the box w.r.t the image size.\n",
    "    box_class_probs: Probability of all the classes for each box.\n",
    "    threshold: Threshold value for box confidence. \n",
    "    \n",
    "    Returns: \n",
    "    scores: containing the class probability score for the selected boxes.\n",
    "    boxes: contains box coordinates for the selected boxes.\n",
    "    classes: contains the index of the class detected by the selected boxes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the box scores: \n",
    "    box_scores = box_confidence * box_class_probs\n",
    "    \n",
    "    # Find the box classes index with the maximum box score\n",
    "    box_classes = K.argmax(box_scores)\n",
    "    # Find the box classes with maximum box score\n",
    "    box_class_scores = K.max(box_scores, axis=-1)\n",
    "    \n",
    "    # Creating a mask for selecting the boxes that have box score greater than threshold.\n",
    "    thresh_mask = box_class_scores >= threshold\n",
    "    # Selecting the scores, boxes and classes with box score greater than \n",
    "    # threshold by filtering the box score with the help of thresh_mask.\n",
    "    scores = tf.boolean_mask(tensor=box_class_scores, mask=thresh_mask)\n",
    "    classes = tf.boolean_mask(tensor=box_classes, mask=thresh_mask)\n",
    "    boxes = tf.boolean_mask(tensor=boxes, mask=thresh_mask)\n",
    "    \n",
    "    return scores, classes, boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(scores, classes, boxes, max_boxes=10, iou_threshold = 0.5):\n",
    "    \"\"\"\n",
    "    Non-maximal suppression is used to fix the multiple detections of the same object.\n",
    "    - Find the box_confidence (Probability of the box containing the object) for each detection.\n",
    "    - Find the bounding box with the highest box_confidence\n",
    "    - Suppress all the bounding boxes which have an IoU greater than 0.5 with the bounding box with the maximum box confidence.\n",
    "    \n",
    "    scores    -> containing the class probability score for the selected boxes.\n",
    "    boxes     -> contains box coordinates for the boxes selected after threshold masking.\n",
    "    classes   -> contains the index of the classes detected by the selected boxes.\n",
    "    max_boxes -> maximum number of predicted boxes to be returned after NMS filtering.\n",
    "    \n",
    "    Returns: \n",
    "    scores  -> predicted score for each box.\n",
    "    classes -> predicted class for each box.\n",
    "    boxes   -> predicted box coordinates.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Converting max_boxes to tensor \n",
    "    max_boxes_tensor = K.variable(max_boxes, dtype='int32')\n",
    "    # Initialize the max_boxes_tensor\n",
    "    K.get_session().run(tf.variables_initializer([max_boxes_tensor]))\n",
    "    \n",
    "    # Implement non-max suppression using tf.image.non_max_suppression()\n",
    "    # tf.image.non_max_suppression() ->  Returns the indicies corresponding to the boxes you want to keep\n",
    "    \n",
    "    indicies = tf.image.non_max_suppression(boxes=boxes, scores=scores, max_output_size=max_boxes_tensor, iou_threshold=iou_threshold)\n",
    "    \n",
    "    # Use K.gather() to select only indicies present in 'indicies' varaible from scores, boxes and classe\n",
    "    \n",
    "    scores = tf.gather(scores, indicies)\n",
    "    classes = tf.gather(classes, indicies)\n",
    "    boxes = tf.gather(boxes, indicies)\n",
    "    \n",
    "    return scores, classes , boxes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_boxes(boxes, image_shape):\n",
    "    \"\"\" Scales the predicted boxes in order to be drawable on the image\"\"\"\n",
    "    height = image_shape[0]\n",
    "    width = image_shape[1]\n",
    "    image_dims = K.stack([height, width, height, width])\n",
    "    image_dims = K.reshape(image_dims, [1, 4])\n",
    "    boxes = boxes * image_dims\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_eval(yolo_outputs, image_shape = (720., 1280.), max_boxes = 10, score_threshold = 0.6, iou_threshold = 0.5):\n",
    "    \"\"\"\n",
    "    The function takes the ouput of the YOLO encoding/ model and filters the boxes using \n",
    "    score threshold and non-maximal suppression. Returns the predicted boxes along with their scores,\n",
    "    box coordinates and classes.\n",
    "    \n",
    "    Arguments: \n",
    "    yolo_outputs    -> Output of the encoding model. \n",
    "    image_shape     -> Input shape \n",
    "    max_boxes       -> Maximum number of predicted boxes to be returned after NMS filtering.\n",
    "    score_threshold -> Threshold value for box class score, if the maximum class probability score < threshold,\n",
    "                       then discard that box. \n",
    "    iou_threshold   -> 'Intersection over Union' threshold used for NMS filtering\n",
    "    \n",
    "    Returns: \n",
    "    scores  -> predicted score for each box.\n",
    "    classes -> predicted class for each box.\n",
    "    boxes   -> predicted box coordinates.\n",
    "    \"\"\"\n",
    "    \n",
    "    box_xy, box_wh, box_confidence, box_class_probs = yolo_outputs\n",
    "    \n",
    "    # Convert boxes to be ready for filtering functions\n",
    "    boxes = yolo_boxes_to_corners(box_xy, box_wh)\n",
    "    \n",
    "    scores, classes, boxes = yolo_filter_boxes(box_confidence, boxes, box_class_probs, score_threshold)\n",
    "    \n",
    "    # Scale boxes back to original image shape.\n",
    "    boxes = scale_boxes(boxes, image_shape)\n",
    "    \n",
    "    # Perform non-max suppression\n",
    "    scores, classes , boxes = non_max_suppression(scores, classes, boxes, max_boxes, iou_threshold)\n",
    "    \n",
    "    return scores, boxes, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo(model_body, class_names, anchors, weights_name='model_data/model.best.h5',):\n",
    "    \"\"\"\n",
    "    Loads the yolo model\n",
    "    \n",
    "    Returns: \n",
    "    scores     -> containing the class probability score for the selected boxes.\n",
    "    boxes      -> contains box coordinates for the boxes selected after threshold masking.\n",
    "    classes    -> contains the index of the classes detected by the selected boxes.\n",
    "    model_body -> the yolo model with the loaded with the save weights\n",
    "    input_image_shape -> Tensor representing the shape of the input image\n",
    "    \"\"\"\n",
    "    model_body.load_weights(weights_name)\n",
    "    yolo_outputs = yolo_head(model_body.output, anchors, len(class_names))\n",
    "    input_image_shape = K.placeholder(shape=(2, ))\n",
    "    scores, boxes, classes = yolo_eval(yolo_outputs, input_image_shape)\n",
    "    \n",
    "    return scores, boxes, classes, model_body, input_image_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(model_body, scores, boxes, classes, input_image_shape, image_data, image_set = 'val', out_path='data/output/', save_all=True, real_time=False):\n",
    "    \"\"\"\n",
    "    Draw the predicted bounding boxes on the image data\n",
    "    \n",
    "    \"\"\"\n",
    "    if image_set == 'real':\n",
    "        image_data = np.expand_dims(image_data, axis=0)\n",
    "    \n",
    "    if image_set == 'val':\n",
    "        image_data = np.array([np.expand_dims(image, axis=0)\n",
    "            for image in image_data[int(len(image_data)*.9):]])\n",
    "    \n",
    "    elif image_set == 'all':\n",
    "        image_data = np.array([np.expand_dims(image, axis=0)\n",
    "            for image in image_data])\n",
    "    \n",
    "    sess = K.get_session()\n",
    "    #if not os.path.exists(out_path):\n",
    "    #    os.makedirs(out_path)\n",
    "    \n",
    "    for i in range(len(image_data)):\n",
    "        out_scores, out_boxes, out_classes = sess.run([scores, boxes, classes], \n",
    "                                                      feed_dict={model_body.input: image_data[i],\n",
    "                                                                 input_image_shape: [image_data.shape[2], image_data.shape[3]],\n",
    "                                                                 K.learning_phase():0\n",
    "                                                                })\n",
    "        print('Found {} boxes for image'.format(len(out_boxes)))\n",
    "        print(out_boxes)\n",
    "        # Generate colors for the drawing bounding boxes\n",
    "        image_with_boxes = draw_boxes(image_data[i][0], out_boxes, out_classes,\n",
    "                                    class_names, out_scores)\n",
    "        \n",
    "        if real_time == True:\n",
    "            return image_with_boxes\n",
    "            \n",
    "        elif save_all or (len(out_boxes) > 0):\n",
    "            image = PIL.Image.fromarray(image_with_boxes)\n",
    "            image.save(os.path.join(out_path,str(i)+'.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the image data and \n",
    "# Loading the box coordinates\n",
    "image_data = np.load('data/image_data.npy')\n",
    "boxes = np.load('data/boxes.npy')\n",
    "boxes = list(boxes)\n",
    "# Selecting the box coordinates of the first 1000 images.\n",
    "boxes = boxes[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-957e32767327>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Preprocess the image data and box coordinates to be fed to the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimage_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mboxes\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-29fbe7ca56ff>\u001b[0m in \u001b[0;36mprocess_data\u001b[1;34m(images, boxes)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprocessed_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m416\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m416\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBICUBIC\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprocessed_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprocessed_images\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mprocessed_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255.\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprocessed_images\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mboxes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-29fbe7ca56ff>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprocessed_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m416\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m416\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBICUBIC\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprocessed_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprocessed_images\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mprocessed_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255.\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprocessed_images\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mboxes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Preprocess the image data and box coordinates to be fed to the model\n",
    "image_data, boxes =  process_data(image_data, boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading anchors and classes\n",
    "anchors= YOLO_ANCHORS\n",
    "class_names = get_classes('data/classes.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-d3d785bffb9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Extracting detector mask and matching true boxes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdetectors_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatching_true_boxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_detector_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-bb0e454ca3e5>\u001b[0m in \u001b[0;36mget_detector_mask\u001b[1;34m(boxes, anchors)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mmatching_true_boxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mdetectors_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatching_true_boxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_true_boxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m416\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m416\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetectors_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatching_true_boxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Desktop\\object-detection-using-yolo-master\\object-detection-using-yolo-master\\model\\YAD2K\\yad2k\\models\\keras_yolo.py\u001b[0m in \u001b[0;36mpreprocess_true_boxes\u001b[1;34m(true_boxes, anchors, image_size)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[0mconv_height\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mheight\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[0mconv_width\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwidth\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m     \u001b[0mnum_box_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrue_boxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m     detectors_mask = np.zeros(\n\u001b[0;32m    387\u001b[0m         (conv_height, conv_width, num_anchors, 1), dtype=np.float32)\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Extracting detector mask and matching true boxes\n",
    "detectors_mask, matching_true_boxes = get_detector_mask(boxes, anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the yolo model with pre-trained weights\n",
    "model_body, model = create_model(anchors, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment to train the network\n",
    "# Training the pre-trained yolo model our image dataset of 1000 images.\n",
    "# train(model, class_names, anchors, image_data, boxes, detectors_mask, matching_true_boxes, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set is the 10% of the image_data\n",
    "scores, boxes, classes, model_body, input_image_shape = load_yolo(model_body, class_names, anchors)\n",
    "# Predicting the classes and box coordinates for the input image\n",
    "draw(model_body, scores, boxes, classes,input_image_shape, image_data, image_set='val',save_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection on test image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the path of the test data\n",
    "test = glob('data/test/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and storing the test image data\n",
    "test_data = []\n",
    "for i in test:\n",
    "    test_data.append(plt.imread(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the test image data \n",
    "test_data = process_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the scores, boxes, classes for the given input image\n",
    "scores, boxes, classes, model_body, input_image_shape = load_yolo(model_body, class_names, anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing the bounding boxes\n",
    "draw(model_body, scores, boxes, classes,input_image_shape, test_data, image_set='all', out_path='data/test/output/',save_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path of the stored video file\n",
    "videopath = 'data/real_time/bdd-videos-sample.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, boxes, classes, model_body, input_image_shape = load_yolo(model_body, class_names, anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = cv2.VideoCapture(videopath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    check, frame = vc.read()\n",
    "    frame = process_data(np.expand_dims(frame, axis=0))\n",
    "    img_data = draw(model_body, scores, boxes, classes, input_image_shape, frame, image_set='real', save_all=False, real_time=True)\n",
    "    img_data = np.array(img_data)\n",
    "    cv2.imshow('Capture:', img_data)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "vc.release()\n",
    "cv2.destroyAllWindows()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
